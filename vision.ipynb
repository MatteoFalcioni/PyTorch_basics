{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b03ccc2a",
      "metadata": {
        "id": "b03ccc2a"
      },
      "source": [
        "# Introduction to Computer Vision with PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. What is Computer Vision?\n",
        "\n",
        "Computer vision is a field of artificial intelligence (AI) that enables machines to interpret and make decisions based on visual data - such as images and videos - much like humans do. From recognizing faces in photos to enabling self-driving cars to detect pedestrians, computer vision is at the heart of many modern applications. It combines image processing, pattern recognition, and deep learning to extract meaningful information from raw pixel data."
      ],
      "metadata": {
        "id": "T4y6qct4Lxnx"
      },
      "id": "T4y6qct4Lxnx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using Convolutional Neural Networks for our tasks. We are not really going to go into the details of the architectures and functioning of these networks - it's out of the scope of this course - but you can find details at **entropic theory of information**.\n"
      ],
      "metadata": {
        "id": "FrAHDlFaMHLT"
      },
      "id": "FrAHDlFaMHLT"
    },
    {
      "cell_type": "markdown",
      "id": "e762fcba",
      "metadata": {
        "id": "e762fcba"
      },
      "source": [
        "Before starting to code for real, let's have a look at some important libraries we may want to use for computer vision problems.\n",
        "\n",
        "| PyTorch module              | What does it do?                                                                                                                                                             |\n",
        "|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| [`torchvision`](https://docs.pytorch.org/vision/stable/index.html#torchvision)              | Contains datasets, model architectures and image transformations often used for computer vision problems.                                                                  |\n",
        "| [`torchvision.datasets`](https://docs.pytorch.org/vision/stable/datasets.html#datasets)     | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |\n",
        "| [`torchvision.models`](https://docs.pytorch.org/vision/stable/models.html#models-and-pre-trained-weights)       | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.                |\n",
        "| [`torchvision.transforms`](https://docs.pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images)   | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check versions\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "cZ5oAmRhO88L"
      },
      "id": "cZ5oAmRhO88L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Get a Dataset\n",
        "\n",
        "We want to use the [`FashionMNIST`](https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#fashionmnist) dataset from `torchvision.datasets`"
      ],
      "metadata": {
        "id": "ySJxfzeUPulj"
      },
      "id": "ySJxfzeUPulj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True, # do we want the training datasets or the test?\n",
        "    download=True, # do we want to download it?\n",
        "    transform=ToTensor(),  # what kind of transformation do we want to apply on data?\n",
        "    target_transform=None # how do we want to transform the labels (targets) ?\n",
        ")"
      ],
      "metadata": {
        "id": "UP4pRbC6RCWz"
      },
      "id": "UP4pRbC6RCWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=False, # do we want the training datasets or the test?\n",
        "    download=True, # do we want to download it?\n",
        "    transform=ToTensor(),  # what kind of transformation do we want to apply on data?\n",
        "    target_transform=None # how do we want to transform the labels (targets) ?\n",
        ")"
      ],
      "metadata": {
        "id": "JPVHCAEuRnE-"
      },
      "id": "JPVHCAEuRnE-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out our data"
      ],
      "metadata": {
        "id": "9V2R3oczRuuj"
      },
      "id": "9V2R3oczRuuj"
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "kPQeZjn8R73Y"
      },
      "id": "kPQeZjn8R73Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the first training example\n",
        "image, label = train_data[0]\n",
        "image"
      ],
      "metadata": {
        "id": "LbSOIJ4_R-NP"
      },
      "id": "LbSOIJ4_R-NP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like the `ToTensor()` transform did its work."
      ],
      "metadata": {
        "id": "zyW6watgSIOC"
      },
      "id": "zyW6watgSIOC"
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "C7AzQXUFSU4_"
      },
      "id": "C7AzQXUFSU4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "FmHbkyDvSl7a"
      },
      "id": "FmHbkyDvSl7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "X7QNwaBbSrJW"
      },
      "id": "X7QNwaBbSrJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shapes\n",
        "print(f\"Image shape: {image.shape} <- [color_channels, height, width];\\nImage label: {label}\")"
      ],
      "metadata": {
        "id": "QaA3VlnUSv5C"
      },
      "id": "QaA3VlnUSv5C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **(!!) Note:** PyTorch uses the **CHW** format for images, meaning the dimensions are ordered as **Channels × Height × Width**. When working with batches of images, the format becomes **NCHW**, where **N** is the batch size. This is different from many image libraries like PIL or OpenCV, which use **HWC** format. Make sure to convert your image tensors accordingly when feeding them into a model.\n"
      ],
      "metadata": {
        "id": "38xabZK5SzCc"
      },
      "id": "38xabZK5SzCc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why just one color? Well, the MNIST dataset is composed of black and white images, so just one channel."
      ],
      "metadata": {
        "id": "DAO-MbUATH1h"
      },
      "id": "DAO-MbUATH1h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Example of encoding an RGB image](https://github.com/MatteoFalcioni/PyTorch_basics/blob/main/imgs/03-fashion-mnist-slide.png?raw=1)"
      ],
      "metadata": {
        "id": "GpCm9AITUGSz"
      },
      "id": "GpCm9AITUGSz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualize our Data"
      ],
      "metadata": {
        "id": "dw7hjTLmU2B5"
      },
      "id": "dw7hjTLmU2B5"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.permute(1,2,0))  # change from CHW -> HWC (torch format -> python format)\n",
        "plt.title(label)"
      ],
      "metadata": {
        "id": "EE-mYdxPVKbW"
      },
      "id": "EE-mYdxPVKbW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in gray scale\n",
        "plt.imshow(image.permute(1,2,0), cmap=\"gray\")\n",
        "plt.title(f\"{label} : {class_names[label]}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "tkem7qdAVz2q"
      },
      "id": "tkem7qdAVz2q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more (random) images\n",
        "torch.manual_seed(123)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows*cols+1):\n",
        "  random_idx  = torch.randint(0, len(train_data), size=[1]).item()\n",
        "  img, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(img.permute(1,2,0), cmap=\"gray\")\n",
        "  plt.title(f\"{label} : {class_names[label]}\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "hu5mNKEKWi17"
      },
      "id": "hu5mNKEKWi17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare a Dataloader"
      ],
      "metadata": {
        "id": "iCqDreiSXZTK"
      },
      "id": "iCqDreiSXZTK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, a [`Dataloader`](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html#datasets-dataloaders) is a powerful utility that provides an efficient way to iterate over a dataset. It wraps a dataset object (typically a subclass of `torch.utils.data.Dataset`) and returns data in manageable **batches**, optionally shuffling the data and loading it in parallel using multiple worker processes.\n",
        "\n",
        "DataLoaders are Python iterables, which means you can loop over them using a `for` loop to retrieve batches of data. Each iteration yields a tuple (usually `(inputs, labels)`) that can be fed into a model for training or evaluation. By handling batching, shuffling, and multiprocessing under the hood, DataLoaders streamline the data pipeline and allow you to focus on model development.\n",
        "\n",
        "We said that we want to turn our model into *batches* (or minibatches). Why is that? Because\n",
        "\n",
        "1. The memory resources of a machine are limited, and it generally can't manage to process all the dataset at once, so deviding into batches is more computionally efficient.\n",
        "\n",
        "2. It gives our neural network more chances to update the gradient at each epoch.\n",
        "\n",
        "We want to create batches of size $32$ - this is a customizable number, you could use $16$, $64$, $8$..."
      ],
      "metadata": {
        "id": "rH9B7_xwYksb"
      },
      "id": "rH9B7_xwYksb"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# batch size\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# turn datasets into iterable (batches)\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True) # shuffle train data to mix order\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False) # no need to shuffle eval data"
      ],
      "metadata": {
        "id": "zuxYQaFd36Ln"
      },
      "id": "zuxYQaFd36Ln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what's inside the Datsaloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader)) # since our dataloader\n",
        "train_features_batch.shape, train_labels_batch.shape\n"
      ],
      "metadata": {
        "id": "kaP8Pl-X6A6M"
      },
      "id": "kaP8Pl-X6A6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.permute(1,2,0), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "id": "U37kzcuO7LCu"
      },
      "id": "U37kzcuO7LCu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build a baseline model\n",
        "\n",
        "A baseline model is a simple model you will try and improve upon with subsequent models/ experiments (starts simple, then add complexity)."
      ],
      "metadata": {
        "id": "RwzEKhG16rnl"
      },
      "id": "RwzEKhG16rnl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "\n",
        "x = train_features_batch[0]\n",
        "x.shape"
      ],
      "metadata": {
        "id": "pVFxSa668qN6"
      },
      "id": "pVFxSa668qN6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the sample...\n",
        "output = flatten_model(x)\n",
        "\n",
        "print(f\"Shape before flattening: {x.shape} -> shape after flattening: {output.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YIDgXHWm9gST"
      },
      "id": "YIDgXHWm9gST",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `nn.Flatten()` layer literally flattens a contiguous range of dims into a tensor. By default it starts by dim=1 (not 0) and ends at the last. So if we squeeze we get"
      ],
      "metadata": {
        "id": "qNJRiub29nbw"
      },
      "id": "qNJRiub29nbw"
    },
    {
      "cell_type": "code",
      "source": [
        "(output.squeeze()).shape"
      ],
      "metadata": {
        "id": "bORwEE-G-V_l"
      },
      "id": "bORwEE-G-V_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use it in order to feed this flattened output to successive linear layers, since these expect vectors, not 2D grids."
      ],
      "metadata": {
        "id": "g5OOiq5CCKRZ"
      },
      "id": "g5OOiq5CCKRZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create the V0 model:\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape : int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)\n"
      ],
      "metadata": {
        "id": "IbZeWkTL-a2P"
      },
      "id": "IbZeWkTL-a2P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_0 = FashionMNISTModelV0(input_shape=784,  # 28*28\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "wpW3UUBo-2sZ"
      },
      "id": "wpW3UUBo-2sZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28]).to(device)  # 1 example of shape 1x28x28\n",
        "model_0(dummy_x).shape  # 1 logit per class"
      ],
      "metadata": {
        "id": "0zfyX1kZBOYI"
      },
      "id": "0zfyX1kZBOYI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup  loss, optimizer and metrics\n",
        "\n",
        "* Since we are in multiclass classification we will use `CrossEntropyLoss()`\n",
        "* For the optimizer we'll stick with `torch.optim.SGD()`\n",
        "* Evaluation metric: since we are facing a classification problem, we will use accuracy."
      ],
      "metadata": {
        "id": "4hHGO-kT-2ax"
      },
      "id": "4hHGO-kT-2ax"
    },
    {
      "cell_type": "code",
      "source": [
        "# We could use torchmetrics but let's use a custom one for now, to see how we import from github\n",
        "\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "59n3EGX3DDr1"
      },
      "id": "59n3EGX3DDr1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup Loss and Optimizer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)\n"
      ],
      "metadata": {
        "id": "NxJBLmQ1DjOI"
      },
      "id": "NxJBLmQ1DjOI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating a function to time our experiments\n",
        "\n",
        "We usually want to time how long our training takes:"
      ],
      "metadata": {
        "id": "woN8EUAn-ViR"
      },
      "id": "woN8EUAn-ViR"
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float,\n",
        "                     end : float,\n",
        "                     device : torch.device=None):\n",
        "  \"\"\"\n",
        "  Prints difference between start and end time\n",
        "  \"\"\"\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "eZxuTd2D9ZFh"
      },
      "id": "eZxuTd2D9ZFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "# some code ...\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time, end=end_time, device=device)"
      ],
      "metadata": {
        "id": "5L95Nf7D98CI"
      },
      "id": "5L95Nf7D98CI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training Loop"
      ],
      "metadata": {
        "id": "TZqKgeMF-dQr"
      },
      "id": "TZqKgeMF-dQr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# set the seed\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "9cS6t-Q1AMht"
      },
      "id": "9cS6t-Q1AMht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, test_dataloader, num_epochs, loss_fn, optimizer, device):\n",
        "  \"\"\"\n",
        "  Trains and tests the given model, by:\n",
        "  1. Looping through epochs\n",
        "  2. Looping through training batches -> performs training step -> calculates train loss *per batch*\n",
        "  3. Looping through test batches -> performs test step -> calculates test loss *per batch*\n",
        "\n",
        "  Args:\n",
        "    - model (torch.nn.Module): the model to train\n",
        "    - train_dataloader (torch.utils.data.Dataloader): torch Dataloader containing training data\n",
        "    - test_dataloader (torch.utils.data.Dataloader): torch Dataloader containing test data\n",
        "    - num_epochs (int): number of epochs to train the model for\n",
        "    - device () : the device to run the training on\n",
        "\n",
        "  Returns:\n",
        "    - train_losses, test_losses, accuracies (lists): lists containing training loss, test loss and accuracies, averaged over batches\n",
        "\n",
        "  \"\"\"\n",
        "  # to store data\n",
        "  train_losses, test_losses, accuracies = [], [], []\n",
        "\n",
        "  # Train and test loop\n",
        "  for epoch in tqdm(range(num_epochs)): # loop through epochs\n",
        "\n",
        "    print(f\"------------\\nEpoch [{epoch}/{num_epochs}]\")\n",
        "\n",
        "    train_loss = 0  # to accumulate loss *per batch*\n",
        "\n",
        "    ## Train loop through batches\n",
        "    for batch, (X_train, y_train) in enumerate(train_dataloader):\n",
        "\n",
        "      X_train = X_train.to(device)\n",
        "      y_train = y_train.to(device)\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      # 1. forward pass\n",
        "      y_pred = model(X_train)\n",
        "\n",
        "      # 2. compute loss\n",
        "      loss = loss_fn(y_pred, y_train)\n",
        "      train_loss += loss  # accumulate train loss\n",
        "\n",
        "      # 3. zero out the gradient\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. step the optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch % 400 == 0:\n",
        "        print(f\"Processed {batch * len(X_train) / len(train_dataloader.dataset):.3f}% of samples\")\n",
        "\n",
        "    # Divide total train loss by the length of the train dataloader to average over batches\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ## Testing\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "      ## Test loop through batches\n",
        "      for X_test, y_test in test_dataloader:\n",
        "        X_test = X_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        test_pred = model(X_test)\n",
        "\n",
        "        # Compute and cumulate loss\n",
        "        test_loss += loss_fn(test_pred, y_test)\n",
        "        accuracy += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=-1)) # careful, preds need argmax to go from logits to preds\n",
        "\n",
        "      # average test loss and accuracy over batches\n",
        "      test_loss /= len(test_dataloader)\n",
        "      accuracy /= len(test_dataloader)\n",
        "\n",
        "    # keep track of losses and accuracy over epochs\n",
        "    train_losses.append(train_loss.item())\n",
        "    test_losses.append(test_loss.item())\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\">>> Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "  return train_losses, test_losses, accuracies\n"
      ],
      "metadata": {
        "id": "jBIIpkAbD8jQ"
      },
      "id": "jBIIpkAbD8jQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train!\n",
        "\n",
        "NUM_EPOCHS = 4\n",
        "\n",
        "start_time = timer()\n",
        "train_losses, test_losses, accuracies = train_model(model=model_0,\n",
        "                                                    train_dataloader=train_dataloader,\n",
        "                                                    test_dataloader=test_dataloader,\n",
        "                                                    num_epochs=NUM_EPOCHS,\n",
        "                                                    loss_fn=loss_fn,\n",
        "                                                    optimizer=optimizer,\n",
        "                                                    device=device)\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time, end=end_time, device=device)"
      ],
      "metadata": {
        "id": "89-ULEyfHwnd"
      },
      "id": "89-ULEyfHwnd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_losses(train_losses, test_losses, start_epoch=1):\n",
        "  \"\"\"\n",
        "  Prints losses and accuracy\n",
        "  \"\"\"\n",
        "\n",
        "  num_epochs = range(start_epoch, len(train_losses) + start_epoch)\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.title(\"Train Loss vs Test Loss\")\n",
        "  plt.plot(num_epochs, train_losses, label=\"Train loss\")\n",
        "  plt.plot(num_epochs, test_losses, label=\"Test loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss values\")\n",
        "  plt.legend()\n"
      ],
      "metadata": {
        "id": "qtAePl1qIgIs"
      },
      "id": "qtAePl1qIgIs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_losses(train_losses, test_losses)"
      ],
      "metadata": {
        "id": "0jdFeARBLEAE"
      },
      "id": "0jdFeARBLEAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We slightly overfitted towards the end of training... but let's compute predicitons anyway to see if our model is outputting something good."
      ],
      "metadata": {
        "id": "ce_bu7MwGOdH"
      },
      "id": "ce_bu7MwGOdH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions\n",
        "\n",
        "We will functionize the evaluation loop as well, in order to be able to use it for the next models."
      ],
      "metadata": {
        "id": "WJUwrJ1KGVji"
      },
      "id": "WJUwrJ1KGVji"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "def eval_model(model: nn.Module,\n",
        "               eval_dataloader: DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device):\n",
        "  \"\"\"\n",
        "  Returns a dictionary of model predicting on eval_dataloader\n",
        "  \"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    for X, y in eval_dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Accumulate loss and accuracy per batch\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y,\n",
        "                         y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Average loss and accuracy over batches\n",
        "    loss /= len(eval_dataloader)\n",
        "    acc /= len(eval_dataloader)\n",
        "\n",
        "  return{\"model_name\": model.__class__.__name__,  # works when model was created with a class\n",
        "         \"model_loss\": loss.item(),\n",
        "         \"model_acc\": acc}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CJwZQ-QLGgfR"
      },
      "id": "CJwZQ-QLGgfR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate model 0 results on test dataset\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             eval_dataloader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device)\n",
        "\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "Sdb8RJKbIwIY"
      },
      "id": "Sdb8RJKbIwIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Improvements: Add Non-Linearity\n",
        "\n",
        "We have good results, but trust me, we can do way better. Let's add non-linearity to our code."
      ],
      "metadata": {
        "id": "XkhNZiZlJXSA"
      },
      "id": "XkhNZiZlJXSA"
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               output_shape: int,\n",
        "               hidden_units: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),  # don't forget to flatten!\n",
        "        nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "29Pj3OnAJ1Iw"
      },
      "id": "29Pj3OnAJ1Iw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_1 = FashionMNISTModelV1(input_shape=784,\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)\n",
        "\n",
        "model_1"
      ],
      "metadata": {
        "id": "xYljZWaeLN6p"
      },
      "id": "xYljZWaeLN6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Functionize Everything\n",
        "\n",
        "Let's functionize once and for good our training and testing loops:"
      ],
      "metadata": {
        "id": "f8YYnONAV6or"
      },
      "id": "f8YYnONAV6or"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               train_loader: DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "  \"\"\"\n",
        "  Performs a training step with a given model\n",
        "  \"\"\"\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "\n",
        "  # training mode\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X, y) in enumerate(train_loader):\n",
        "\n",
        "    # put data on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    # accumulate loss and acc\n",
        "    train_loss += loss.item()\n",
        "    train_acc += accuracy_fn(y_true=y,\n",
        "                             y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # zero out the gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  # average loss and accuracy over samples\n",
        "  train_loss /= len(train_loader)\n",
        "  train_acc /= len(train_loader)\n",
        "\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\\n\")\n",
        "\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "5k43Bb0WLx5A"
      },
      "id": "5k43Bb0WLx5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              test_loader: DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "  \"\"\"\n",
        "  performs a testing loop step on a given model\n",
        "  \"\"\"\n",
        "\n",
        "  # eval mode\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  test_acc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "\n",
        "      # send to device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # get predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # accumulate losses\n",
        "      test_loss += loss_fn(y_pred, y).item()\n",
        "      test_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # average metrics\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc /= len(test_loader)\n",
        "\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "7waWeTEPWAFF"
      },
      "id": "7waWeTEPWAFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Dict, List, Tuple\n",
        "\n",
        "def train_epochs(model: torch.nn.Module,\n",
        "                 train_loader: DataLoader,\n",
        "                 test_loader: DataLoader,\n",
        "                 loss_fn: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 accuracy_fn : Callable[[torch.Tensor, torch.Tensor], float],\n",
        "                 device: torch.device,\n",
        "                 n_epochs: int\n",
        "                 ) -> Dict[str, List[float]]:\n",
        "  \"\"\"\n",
        "  Performs training and evaluation of the given model during several epochs.\n",
        "  \"\"\"\n",
        "\n",
        "  # lists to store metrics\n",
        "  train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], []\n",
        "\n",
        "  for epoch in tqdm(range(n_epochs), desc=\"Training and testing...\"):\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} ------\")\n",
        "    # training step\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                        train_loader=train_loader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        optimizer=optimizer,\n",
        "                                        accuracy_fn=accuracy_fn,\n",
        "                                        device=device)\n",
        "\n",
        "    # test step\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    test_loader=test_loader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    accuracy_fn=accuracy_fn,\n",
        "                                    device=device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "  # return a dictionary of metrics\n",
        "  return {\"train_loss\": train_losses,\n",
        "          \"train_acc\": train_accuracies,\n",
        "          \"test_loss\": test_losses,\n",
        "          \"test_acc\": test_accuracies}\n",
        "\n"
      ],
      "metadata": {
        "id": "jjd5L6ggXwAA"
      },
      "id": "jjd5L6ggXwAA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = FashionMNISTModelV1(input_shape=784,\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)\n",
        "\n",
        "optimizer=torch.optim.SGD(params=model_1.parameters(),\n",
        "                          lr=0.01)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "NUM_EPOCHS = 4\n",
        "\n",
        "metrics_dict = train_epochs(model=model_1,\n",
        "                            train_loader=train_dataloader,\n",
        "                            test_loader=test_dataloader,\n",
        "                            loss_fn=loss_fn,\n",
        "                            optimizer=optimizer,\n",
        "                            accuracy_fn=accuracy_fn,\n",
        "                            device=device,\n",
        "                            n_epochs=NUM_EPOCHS)\n",
        "\n",
        "metrics_dict"
      ],
      "metadata": {
        "id": "CvbDYihiZ3Pi"
      },
      "id": "CvbDYihiZ3Pi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait, what? We didn't surpass the non linear model! Hhhmmmm... maybe just using non-linearity is not enough for more complex problems?\n",
        "\n",
        "We will now try and implement a more complex network: a Convolutional Neural Network (CNN)."
      ],
      "metadata": {
        "id": "QIbuobPsbFyC"
      },
      "id": "QIbuobPsbFyC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Building a Convolutional Neural Network\n",
        "\n",
        "If you want a quick explanation of how CNNs work, look [here](https://poloclub.github.io/cnn-explainer/). For a deeper dive, refer to *entropic theory of information*.\n",
        "\n",
        "Below you can see the usual structure of a CNN, with its funnel-like architecture for feature extraction.\n",
        "\n"
      ],
      "metadata": {
        "id": "bNvoH_WfmSSv"
      },
      "id": "bNvoH_WfmSSv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Example of encoding an RGB image](https://github.com/MatteoFalcioni/PyTorch_basics/blob/main/imgs/convnet.png?raw=1)"
      ],
      "metadata": {
        "id": "fkNafwBTNO5K"
      },
      "id": "fkNafwBTNO5K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start to code our CNN. It will be a \"deep\" CNN, i.e., composed os more than 1 convolutional block.\n",
        "\n",
        "For now we'll just code it by leaving out the adjustements to tensor shapes that a CNN architecture requires, then we'll investigate the changes later on."
      ],
      "metadata": {
        "id": "nGVKA5IiNMFc"
      },
      "id": "nGVKA5IiNMFc"
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelCNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Model replicating TinyVGG\n",
        "  2 convolutional blocks + final linear layer\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # first convolutional block\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # second convolutional block\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # final linear layer\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*0, # we'll leave it to *0 for now\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.conv_block_1(x)\n",
        "    print(x.shape)\n",
        "    x = self.conv_block_2(x)\n",
        "    print(x.shape)\n",
        "    x = self.classifier(x)\n",
        "    print(x.shape)\n",
        "    return x"
      ],
      "metadata": {
        "id": "0gko_M47GJyg"
      },
      "id": "0gko_M47GJyg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# instantiate the model\n",
        "model_2 = FashionMNISTModelCNN(input_shape=1,\n",
        "                               hidden_units=10,\n",
        "                               output_shape=len(class_names)).to(device)\n",
        "model_2"
      ],
      "metadata": {
        "id": "fGvwV9SuJaDc"
      },
      "id": "fGvwV9SuJaDc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Stepping through `nn.Conv2d`"
      ],
      "metadata": {
        "id": "f11NV5JzK_kW"
      },
      "id": "f11NV5JzK_kW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the documentation for Conv2d [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d)."
      ],
      "metadata": {
        "id": "yw-Hg0xzLr_h"
      },
      "id": "yw-Hg0xzLr_h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test how shapes work in convolution  by using a dummy tensor."
      ],
      "metadata": {
        "id": "1xXEdrJbMGOT"
      },
      "id": "1xXEdrJbMGOT"
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data = torch.rand(32, 3, 64, 64)  # Batch x Channels x Heigth x Width (N x C x H x W)\n",
        "dummy_img = dummy_data[0]\n",
        "dummy_img.shape"
      ],
      "metadata": {
        "id": "HgJZquP2MVz9"
      },
      "id": "HgJZquP2MVz9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=(3,3), # equivalent to kernel_size=3\n",
        "                       stride=1,\n",
        "                       padding=0)\n",
        "conv_output = conv_layer(dummy_img)\n",
        "conv_output.shape"
      ],
      "metadata": {
        "id": "HKzbez7KMdBJ"
      },
      "id": "HKzbez7KMdBJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ayo? we got a differrent number of channels AND a different number of pixels.\n",
        "\n",
        "No worries: this is exactly what convolutional layers do. We actually have a formula to compute what the ouput size is depending on kernel size, padding and stride.\n",
        "\n",
        "**Convolution Output Size Formula**\n",
        "\n",
        "Given:\n",
        "- Input size: `i`\n",
        "- Kernel size: `k`\n",
        "- Padding: `p` (on each side)\n",
        "- Stride: `s`\n",
        "\n",
        "The output size `o` is computed as:\n",
        "\n",
        "$$\n",
        "o = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "notice that we're taking the integer value of with the floor operation.\n",
        "\n",
        "So in our case:\n",
        "* $i=64$\n",
        "* $k=3$\n",
        "* $p=0$\n",
        "* $s=1$\n",
        "\n",
        "Therefore\n",
        "$$\n",
        "o = \\left\\lfloor\\frac{64 + 2*0 - 3}{1} \\right\\rfloor + 1 = 61 +1 = 62\n",
        "$$\n",
        "\n",
        "\n",
        "This formula is of course different if we apply a \"rectangular\" kernel, i.e. we perform different convolutional operations on different dimensions. In our case we are using a square kernel (as in most cases)."
      ],
      "metadata": {
        "id": "VTD8A3svOPeZ"
      },
      "id": "VTD8A3svOPeZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Stepping through `MaxPool2d`\n",
        "\n",
        "What about `MaxPool2d`?"
      ],
      "metadata": {
        "id": "MjKeyUWjPryl"
      },
      "id": "MjKeyUWjPryl"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"convolutional output shape: {conv_output.shape}\")\n",
        "\n",
        "# create a maxpool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "pooled_img = max_pool_layer(conv_output)\n",
        "print(f\"shape of convolved and pooled imag: {pooled_img.shape}\")"
      ],
      "metadata": {
        "id": "X1rk5hr-RdaA"
      },
      "id": "X1rk5hr-RdaA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmmm, what happened?\n",
        "\n",
        "Well, the pooling operation is the operation that spatially reduces the dimensions of our tensors in convolutional blocks..\n",
        "\n",
        "Essentially, the pooling operation is just a sliding window operation without learnable weights. So it's formula is identical to the convloutional one we saw above.\n",
        "\n",
        "We just passed `kernel_size=2` because we wanted to reduce the dimension by half in this example, so that's the easiest way to do it.\n",
        "\n",
        "So, for example:"
      ],
      "metadata": {
        "id": "PSK9dQ4JR-sg"
      },
      "id": "PSK9dQ4JR-sg"
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.rand(3, 128, 128)\n",
        "new_max_pool_layer = nn.MaxPool2d(kernel_size=4)\n",
        "pooled_image = new_max_pool_layer(image)\n",
        "pooled_image.shape"
      ],
      "metadata": {
        "id": "ovMlUUmUUEhs"
      },
      "id": "ovMlUUmUUEhs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Max pooling is not the only one we can use. there are several common types of pooling\n",
        "\n",
        "1. **Max Pooling**\n",
        "- Picks the **maximum value** in each window.\n",
        "- Commonly used in `nn.MaxPool2d`.\n",
        "\n",
        "2. **Average Pooling**\n",
        "- Computes the **average value** in each window.\n",
        "- PyTorch: `nn.AvgPool2d`\n",
        "\n",
        "3. **Global Average Pooling**\n",
        "- Averages over the **entire spatial dimension**, reducing `(C, H, W)` → `(C, 1, 1)`.\n",
        "- Used to flatten features before classification.\n",
        "\n",
        "4. **Adaptive Pooling**\n",
        "- Output size is specified, kernel/stride is automatically computed.\n",
        "- PyTorch: `nn.AdaptiveAvgPool2d((H_out, W_out))`\n",
        "- Useful when input size varies (e.g., object detection, classification heads).\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔧 When to Use Pooling\n",
        "\n",
        "- To **reduce memory & computation**\n",
        "- To **prevent overfitting** via spatial abstraction\n",
        "- To **keep important features** while discarding noise\n",
        "\n",
        "> ⚠️ Modern architectures (like ResNets or Transformers) often use **strided convolutions** or **global pooling** instead of repeated max pooling."
      ],
      "metadata": {
        "id": "YHx_6dbTUE0Z"
      },
      "id": "YHx_6dbTUE0Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Getting the CNN shapes right\n",
        "\n",
        "Now our question is: can our input correctly pass through the Convolutional model we created?\n",
        "\n",
        "We could use different methods to check it. We now know the equations that determine the output shapes of convolutional and pooling layers, so we might compute them and adjust them in our model class.\n",
        "\n",
        "Or, we can do this \"empirically\", by using a dummy input of correct size - i.e., `[1, 28,28]` - and trying to pass it through our model.\n",
        "\n",
        "Let's use the latter approach, which is actually pretty useful when building complex architecture in order to understand them better."
      ],
      "metadata": {
        "id": "0YPQq06UYCEz"
      },
      "id": "0YPQq06UYCEz"
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.rand(32, 1, 28, 28).to(device)  # dummy batch size = 32\n",
        "model_2(dummy_input)"
      ],
      "metadata": {
        "id": "_XNm5P3_YUhZ"
      },
      "id": "_XNm5P3_YUhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, so basically we encountered a problem in our final linear layer. There is a shape mismatch between `32x490` and `0x10`. That `0` is actually the value we hardcoded in order to experiment with shapes.\n",
        "\n",
        "Now, the output of `conv_block2` is being flattened and then passed through the linear layer.\n",
        "\n",
        "What would be the flattened shape? Well, we are flattening a tensor of shape `torch.Size([32, 10, 7, 7])` starting from dimension $1$, so we would get a flattened tensor of size $10*7*7 = 490$.\n",
        "\n",
        "Therefore our linear layer should take that shape as input."
      ],
      "metadata": {
        "id": "5h_4KES6YfG2"
      },
      "id": "5h_4KES6YfG2"
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelCNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Model replicating TinyVGG\n",
        "  2 convolutional blocks + final linear layer\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # first convolutional block: (N, 1, 28, 28) -> (N, hidden_units, 14, 14)\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # second convolutional block: (N, hidden_units, 14, 14) -> (N, hidden_units, 7, 7)\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # final linear layer: (N, hidden_units, 7, 7) -> (N, 10)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),   # hidden_units*7*7\n",
        "        nn.Linear(in_features=hidden_units*7*7, # correct shape\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
      ],
      "metadata": {
        "id": "KQoe-FIDchx2"
      },
      "id": "KQoe-FIDchx2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = FashionMNISTModelCNN(input_shape=1,\n",
        "                               hidden_units=10,\n",
        "                               output_shape=len(class_names)).to(device)\n",
        "model_3"
      ],
      "metadata": {
        "id": "YMAIw_Dcdvyx"
      },
      "id": "YMAIw_Dcdvyx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example:\n",
        "batch, (X, y) = next(enumerate(train_dataloader))\n",
        "X.shape"
      ],
      "metadata": {
        "id": "xaXOb3zjd8XM"
      },
      "id": "xaXOb3zjd8XM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model_3(X.to(device))\n",
        "\n",
        "print(f\"Final output shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "Dbi_ERgFebqL"
      },
      "id": "Dbi_ERgFebqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Train the CNN"
      ],
      "metadata": {
        "id": "7xnsFa0tfwyU"
      },
      "id": "7xnsFa0tfwyU"
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_3.parameters(),\n",
        "                            lr=0.1)\n",
        "\n",
        "NUM_EPOCHS = 4\n",
        "model_3_results = train_epochs(model_3,\n",
        "                              train_dataloader,\n",
        "                              test_dataloader,\n",
        "                              loss_fn,\n",
        "                              optimizer,\n",
        "                              accuracy_fn,\n",
        "                              device,\n",
        "                              NUM_EPOCHS\n",
        "                              )\n"
      ],
      "metadata": {
        "id": "vqFWW6UIf_Q7"
      },
      "id": "vqFWW6UIf_Q7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_losses(model_3_results['train_loss'], model_3_results['test_loss'])"
      ],
      "metadata": {
        "id": "VFB7HmzLf2zA"
      },
      "id": "VFB7HmzLf2zA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_accuracy(train_accuracy, test_accuracy):\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.title(\"Accuracy values over epochs\")\n",
        "  plt.bar(train_accuracy, color=\"green\", label=\"Train Accuracy\")\n",
        "  plt.bar(test_accuracy, color=\"yellow\", label=\"Test Accuracy\")\n"
      ],
      "metadata": {
        "id": "6BtBWxCMg--1"
      },
      "id": "6BtBWxCMg--1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(model_3_results['train_acc'], model_3_results['test_acc'])"
      ],
      "metadata": {
        "id": "8bMqXauoh2YA"
      },
      "id": "8bMqXauoh2YA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We managed to surpass our previous model!\n",
        "\n",
        "But be careful in real life applications: the last model took more time to train, and in"
      ],
      "metadata": {
        "id": "72zYTsaqh8ej"
      },
      "id": "72zYTsaqh8ej"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}